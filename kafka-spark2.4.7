from pyspark.sql import SparkSession
from pyspark.sql.functions import col, from_json
from pyspark.sql.types import StructType, StructField, StringType

# Correct JARs for Spark 2.4.7 (Scala 2.11)
KAFKA_JARS = [
    "org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.7",  # Must use _2.11
    "org.apache.kafka:kafka-clients:2.0.0"                # Compatible client
]

# Initialize Spark with Kafka support
spark = SparkSession.builder \
    .appName("KafkaToParquet_DynamicSchema") \
    .config("spark.jars.packages", ",".join(KAFKA_JARS)) \
    .config("spark.sql.parquet.writeLegacyFormat", "true") \  # For Spark 2.x Parquet
    .getOrCreate()

# 1. Read from Kafka (supports Kafka 0.10+)
kafka_df = spark.readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "localhost:9092") \
    .option("subscribe", "test-topic") \
    .option("startingOffsets", "earliest") \
    .option("failOnDataLoss", "false") \  # Prevents crashes on minor data loss
    .load()

# 2. Convert Kafka binary value to JSON string
json_string_df = kafka_df.select(
    col("value").cast("string").alias("json_string")
)

# 3. Dynamic schema inference (with fallback)
try:
    # Get sample record for schema inference
    sample_record = json_string_df.limit(1).select("json_string").collect()
    if sample_record:
        sample_json = sample_record[0]["json_string"]
        # Infer schema from sample JSON
        schema = spark.read.json(
            spark.sparkContext.parallelize([sample_json])
        ).schema
    else:
        # Fallback schema if no data
        schema = StructType([StructField("raw_json", StringType())])
except Exception as e:
    print(f"Schema inference failed: {e}")
    schema = StructType([StructField("raw_json", StringType())])

# 4. Parse JSON with inferred schema
parsed_df = json_string_df.withColumn(
    "data", 
    from_json(col("json_string"), schema)
).select("data.*")

# 5. Write to Parquet (with checkpointing)
query = parsed_df.writeStream \
    .format("parquet") \
    .outputMode("append") \
    .option("path", "/output/transactions_parquet") \
    .option("checkpointLocation", "/output/checkpoint") \
    .option("maxRecordsPerFile", 100000) \  # Control file size
    .start()

print("Streaming query started...")
query.awaitTermination()
